{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad94c70d",
   "metadata": {},
   "source": [
    "## MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for embeddings, LLM, and environment configuration\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the embedding model and LLM\n",
    "\n",
    "embedding = OllamaEmbeddings(model= \"nomic-embed-text:v1.5\")\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccee9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the embedding model by embedding a sample text\n",
    "r = embedding.embed_documents(\"Bye bye\")\n",
    "len(r[0])  # Print the vector dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MongoDB Atlas connection URI from environment variables\n",
    "# This URI contains credentials for connecting to the MongoDB Atlas cluster\n",
    "import os\n",
    "MONGODB_ATLAS_CLUSTER_URI = os.environ.get(\"MONGODB_ATLAS_CLUSTER_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "927e6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MongoDB Atlas vector store for similarity search\n",
    "# This integrates LangChain with MongoDB Atlas for vector-based document retrieval\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo import  MongoClient\n",
    "\n",
    "# Connect to MongoDB Atlas cluster using the connection URI\n",
    "client = MongoClient(MONGODB_ATLAS_CLUSTER_URI)\n",
    "\n",
    "# Define database and collection names for storing vectors and documents\n",
    "DB_NAME = \"rag\"\n",
    "COLLECTION_NAME = \"langchain_rag_vector_store\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"langchain-test-index-vectorstores\"\n",
    "\n",
    "# Reference the MongoDB collection\n",
    "MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    collection=MONGODB_COLLECTION,\n",
    "    embedding=embedding,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    relevance_score_fn=\"cosine\",\n",
    ")\n",
    "\n",
    "# Create vector search index on the collection (768-dimensional vectors)\n",
    "vector_store.create_vector_search_index(dimensions=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca9a36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "\n",
    "def load_documents(data_dir):\n",
    "    documents = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        if file.endswith(\".pdf\"):\n",
    "            # Load PDF using PyPDFLoader\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            documents.extend(loader.load())\n",
    "        # Note: Support for DOCX files can be added by uncommenting the section below\n",
    "        # elif file.endswith(\".docx\"):\n",
    "        #     loader = Docx2txtLoader(file_path)\n",
    "        #     documents.extend(loader.load())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae958fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all PDF documents from the 'data' directory\n",
    "docs = load_documents(\"data\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d69edd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split documents into smaller chunks for efficient embedding and retrieval\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "len(chunks)  # Display the total number of chunks created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['69331a25e7928100e9416fa0',\n",
       " '69331a25e7928100e9416fa1',\n",
       " '69331a25e7928100e9416fa2',\n",
       " '69331a25e7928100e9416fa3',\n",
       " '69331a25e7928100e9416fa4',\n",
       " '69331a25e7928100e9416fa5',\n",
       " '69331a25e7928100e9416fa6',\n",
       " '69331a25e7928100e9416fa7',\n",
       " '69331a25e7928100e9416fa8',\n",
       " '69331a25e7928100e9416fa9',\n",
       " '69331a25e7928100e9416faa',\n",
       " '69331a25e7928100e9416fab',\n",
       " '69331a25e7928100e9416fac',\n",
       " '69331a25e7928100e9416fad',\n",
       " '69331a25e7928100e9416fae']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add all document chunks to the MongoDB vector store\n",
    "vector_store.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315be5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a test similarity search to verify the vector store is working\n",
    "results = vector_store.similarity_search(\n",
    "    \"RAG\", k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7cc637ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2 RAG Workflow\\n2.2 RAG Workflow\\nThe RAG process follows a systematic workflow. When a user submits a query, the system first converts it into an embedding vector. This query embedding is then\\nused to search the vector database for the most semantically similar document chunks. The retrieved documents are ranked by relevance, and the top-k results are\\nselected as context.\\nThe language model receives both the original query and the retrieved context as input, generating a response that synthesizes information from the provided\\ndocuments. This approach significantly reduces hallucinations and allows the model to cite sources, improving trustworthiness and verifiability.\\n2.3 Advanced RAG Techniques\\n2.3 Advanced RAG Techniques\\nModern RAG implementations employ sophisticated techniques to improve performance. Hybrid search combines dense vector search with traditional keyword-'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the content of the retrieved documents\n",
    "results[0].page_content  # First result\n",
    "results[1].page_content  # Second result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cda07",
   "metadata": {},
   "source": [
    "## LangChain Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d8b8f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG tool as a LangChain tool for the agent to use\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def rag_tool(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve relevant information from the PDF documents.\n",
    "    Use this tool when the user asks factual or conceptual questions\n",
    "    that might be answered from the stored documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question or search query\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - query: The original query\n",
    "            - context: List of relevant document excerpts\n",
    "            - metadata: Metadata (e.g., page numbers) for each document\n",
    "    \"\"\"\n",
    "    # Retrieve top 4 most similar documents for the query\n",
    "    result = vector_store.similarity_search(query, k=4)\n",
    "    context = [doc.page_content for doc in result]\n",
    "    metadata = [doc.metadata for doc in result]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"context\": context,\n",
    "        \"metadata\": metadata,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67486a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [rag_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f9702c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create_agent: Creates a ReAct agent that can use tools\n",
    "# - MemorySaver: Stores agent state for conversation continuity and checkpointing\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07114ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory saver for storing conversation state and agent history\n",
    "# This enables multi-turn conversations and state checkpointing\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18bd9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReAct agent that can use the RAG tool to answer questions\n",
    "# The agent will automatically decide when to use the tool based on the user's query\n",
    "agent = create_agent(\n",
    "    model=llm,                    \n",
    "    tools=tools,                  \n",
    "    checkpointer=memory,          \n",
    "    system_prompt=\"You are a concise assistant. Always base answers on the provided PDF and clearly cite any document excerpts used.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a5513631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configuration for agent execution with a specific thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "76dba245",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": \"Key Components of RAG?\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3741ec77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Key components of a Retrieval‑Augmented Generation (RAG) system**\n",
       "\n",
       "1. **Document Corpus** – the collection of documents, databases, or knowledge bases that contain the information to be retrieved.  \n",
       "2. **Embedding Model** – converts text (both queries and documents) into dense vector representations that capture semantic meaning.  \n",
       "3. **Vector Database** – stores the document embeddings and enables fast similarity search (e.g., Pinecone, Weaviate, Chroma).  \n",
       "4. **Retrieval System** – searches the vector store to find the most relevant document chunks for a given query.  \n",
       "5. **Language Model** – receives the original query plus the retrieved context and generates the final response.\n",
       "\n",
       "These components work together to retrieve relevant knowledge and feed it into a generative model, reducing hallucinations and improving answer verifiability【0†L1-L8】."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the agent's final response (last message) and display it as formatted Markdown\n",
    "# This provides a clean, readable output of the agent's answer with proper formatting\n",
    "r = result['messages'][-1].content\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
